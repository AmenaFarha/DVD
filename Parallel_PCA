import numpy as np
import pandas as pd
from sklearn.decomposition import IncrementalPCA
from sklearn.preprocessing import StandardScaler
import threading

# Load and prepare the data
def load_data():
    file_path = r'C:\Users\afarha\Study\Project DVD\wdbc.csv'
    data = data.select_dtypes(include=[np.number])  # Select only numerical data for PCA
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    return data_scaled

data_scaled = load_data()

# Incremental PCA
n_components = 5  # Adjust based on the specific needs
incremental_pca = IncrementalPCA(n_components=n_components)

# Define the function to perform partial fitting
def partial_fit(start, end):
    incremental_pca.partial_fit(data_scaled[start:end])

# Number of threads
num_threads = 4  # Adjust based on your machine capabilities

# Creating threads
threads = []
batch_size = len(data_scaled) // num_threads
for i in range(num_threads):
    start = i * batch_size
    end = start + batch_size if i != num_threads - 1 else len(data_scaled)
    thread = threading.Thread(target=partial_fit, args=(start, end))
    threads.append(thread)
    thread.start()

# Joining all threads
for thread in threads:
    thread.join()

transformed_data = incremental_pca.transform(data_scaled)

output_path = r'C:\Users\afarha\Study\Project DVD\transformed_data.csv'
pd.DataFrame(transformed_data).to_csv(output_path, index=False)
print(f"Transformed data saved to {output_path}")
